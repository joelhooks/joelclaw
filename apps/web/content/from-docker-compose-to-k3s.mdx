---
title: "From Docker Compose to k3s in One Session"
date: "2026-02-16"
description: "Three containers and a spike that turned into a production migration. Why I moved my personal AI infrastructure to Kubernetes — and the gotchas nobody warns you about."
---

Three Docker Compose services. Redis, Qdrant, Inngest. Running on a Mac Mini for months. Working fine.

So why change anything?

## The honest answer

Docker Compose works until it doesn't. Three containers is fine. But the moment I want to add a health check cron, or schedule a batch embedding job, or run a service on a different machine — I'm writing shell scripts and launchd plists and hoping they don't drift apart.

Kubernetes solves scheduling. It solves health checks. It solves "what's running where." The question was never *if* I'd move — it was whether the overhead was worth it on a single machine.

So I spiked it.

## k3d on Mac, k3s on Linux

k3s is Linux-only. On a Mac, you need a way to run it. The options:

| Approach | What it means |
|---|---|
| **k3d** | k3s inside Docker (Docker Desktop is already running) |
| **microk8s** | Multipass VM (separate from Docker Desktop) — two VMs |
| **OrbStack** | Could replace Docker Desktop + run k8s natively |
| **Colima + kind** | Lightweight Docker replacement + k8s-in-Docker |

k3d wins because Docker Desktop is already running — free for personal use. No new VMs, no new abstractions. Benchmarks confirm k3d has the fastest startup of all the local k8s options. microk8s would've meant running Multipass alongside Docker Desktop — two VMs, more overhead, slower startup, for zero benefit on macOS.

On Linux, none of this matters. k3s runs natively — one curl command, no Docker needed, no wrapper. The portable layer is the k8s manifests. Same `kubectl apply -f k8s/` on both platforms, different on-ramp to get there.

```bash
k3d cluster create joelclaw \
  --servers 1 \
  --port "6379:6379@server:0" \
  --port "6333:6333@server:0" \
  --port "8288:8288@server:0" \
  --k3s-arg "--disable=traefik@server:0" \
  --k3s-arg "--kube-apiserver-arg=service-node-port-range=80-32767@server:0" \
  --wait
```

Nineteen seconds to a running cluster.

## The INNGEST_PORT collision

First deploy. Redis comes up. Qdrant comes up. Inngest crashes immediately.

```
strconv.Atoi: parsing "tcp://10.43.53.131:8288": invalid syntax
```

Classic Kubernetes gotcha that nobody warns you about. When you create a Service named `inngest`, Kubernetes auto-injects environment variables into every pod in the namespace:

```
INNGEST_SERVICE_HOST=10.43.53.131
INNGEST_PORT=tcp://10.43.53.131:8288
```

The Inngest binary has its own `INNGEST_PORT` env var — expects an integer. Gets a URL. Crash.

**Fix**: name the Service `inngest-svc`. The env vars become `INNGEST_SVC_PORT` and the collision disappears. Two characters, thirty minutes of debugging.

**Rule**: never name a k8s Service the same as the binary it runs, if that binary reads its own name as an env var prefix. This probably bites every project that runs a service named after its own software.

## The NodePort range gotcha

Kubernetes default NodePort range is 30000-32767. My services need ports 6379, 6333, and 8288. You have to tell k3s at cluster creation:

```
--kube-apiserver-arg=service-node-port-range=80-32767
```

And here's the thing about k3d: **cluster config is immutable after creation.** Port mappings, k3s args, everything is locked at `cluster create` time. Forget a port or a flag, you delete the cluster and start over.

I deleted and recreated the cluster three times during this migration. Plan your ports before you create.

## The manifests are boring (that's the point)

Each Docker Compose service became a StatefulSet + NodePort Service. Same images, same commands, same health checks. The translation is mechanical:

| Docker Compose | k8s |
|---|---|
| `image: redis:7-alpine` | `image: redis:7-alpine` |
| `volumes: redis_data:/data` | `volumeClaimTemplates` with PVC |
| `ports: "6379:6379"` | NodePort service on 6379 |
| `restart: unless-stopped` | StatefulSet (always restarts) |
| `healthcheck` | `livenessProbe` + `readinessProbe` |

StatefulSets because all three services need stable storage. Redis with AOF, Qdrant with its vector index, Inngest with SQLite — they all need data to survive pod restarts.

## The overhead

| What | Memory |
|---|---|
| k3s control plane + system services | ~587 MB |
| Redis pod | 9 MB |
| Qdrant pod | 273 MB |
| Inngest pod | 46 MB |
| **Total** | **~915 MB** |

Docker Compose was 536 MB for the same three services. The k8s tax is ~380 MB — the control plane, CoreDNS, metrics-server, the local-path provisioner. On a 64 GB machine that's 0.6% overhead for a real orchestrator.

The pods themselves actually use *less* memory inside k3s than they did in Docker Compose. Not sure why. Didn't investigate.

## The cutover

The spike ran alongside Docker Compose — different ports, both stacks operational. Once everything checked out:

1. Stop Docker Compose (free the ports)
2. Recreate k3d cluster with production port mappings
3. Deploy manifests
4. Restart the worker (still runs on the host via launchd)
5. Re-register worker with the new Inngest instance
6. Smoke test: send an event, watch it complete

The worker didn't need a single config change. It still connects to `localhost:6379`, `localhost:6333`, `localhost:8288` — same as before. The ports are just served by k8s NodePorts instead of Docker port bindings.

`docker compose down`. Done.

## What stays on launchd

The system-bus worker runs on the host, not in k8s. It needs direct filesystem access — git operations on the Vault, Whisper transcription using the Mac's GPU, reading and writing to paths all over the machine. Containerizing it would mean mounting half the filesystem into a pod.

Caddy stays on launchd too. It terminates TLS with Tailscale certificates that live on the host.

Both are fine where they are. Not everything needs to be in a cluster.

## What this gets me

`kubectl get pods -n joelclaw` shows everything. Health checks are built in — liveness probes restart crashed services, readiness probes gate traffic. Adding a new service is a manifest, not a shell script.

```
NAME        READY   STATUS    RESTARTS   AGE
inngest-0   1/1     Running   0          2h
qdrant-0    1/1     Running   0          2h
redis-0     1/1     Running   0          2h
```

Three services, three StatefulSets, persistent storage, health monitoring. 915 MB on a 64 GB machine. The [network page](/network) shows the current state, and [ADR-0025](/adrs/0025-k3s-cluster-for-joelclaw-network) has the full decision and spike results.

The whole migration — spike, gotcha debugging, manifest writing, cutover, smoke test — was one session.

---

*This is part of a series about building a personal AI system. Previous: [Inngest is the Nervous System](/inngest-is-the-nervous-system).*
