/**
 * Email inbox cleanup ‚Äî durable Inngest function.
 *
 * Paginates through open Front conversations, uses pi CLI inference
 * to classify each batch (archive vs keep), then archives noise.
 *
 * Triggered by: email/inbox.cleanup
 * ADR-0052: Email Port ‚Äî Hexagonal Architecture
 */

import { inngest } from "../client";
import type { GatewayContext } from "../middleware/gateway";
import { execSync } from "child_process";

const FRONT_API = "https://api2.frontapp.com";
const BATCH_SIZE = 50; // conversations per classification call
const PAGE_SIZE = 100; // Front API page size
const MAX_PAGES = 20; // safety cap: 20 pages √ó 100 = 2000 conversations per run

type ConversationSummary = {
  id: string;
  subject: string;
  from: string;
  email: string;
  date: string;
};

function getApiToken(): string {
  const token = process.env.FRONT_API_TOKEN;
  if (!token) throw new Error("FRONT_API_TOKEN not set");
  return token;
}

async function frontGet(path: string, token: string): Promise<any> {
  const res = await fetch(`${FRONT_API}${path}`, {
    headers: {
      Authorization: `Bearer ${token}`,
      Accept: "application/json",
    },
  });
  if (!res.ok) throw new Error(`Front API ${res.status}: ${await res.text()}`);
  return res.json();
}

/**
 * Classify a batch of conversations using pi CLI inference.
 *
 * Uses `pi --print --no-session --provider anthropic --model sonnet`
 * for fast, cheap classification. No Anthropic API key needed ‚Äî
 * pi handles auth via its own provider config.
 */
function classifyBatch(
  conversations: ConversationSummary[],
): { archive: string[]; keep: string[] } {
  const listing = conversations
    .map(
      (c, i) =>
        `${i + 1}. [${c.id}] From: ${c.from} <${c.email}> | Date: ${c.date} | Subject: ${c.subject}`,
    )
    .join("\n");

  const prompt = `You are triaging Joel's email inbox. Classify each conversation as ARCHIVE or KEEP.

ARCHIVE if:
- Marketing, promotions, sales outreach, cold emails
- Automated notifications nobody reads (CI failures, usage alerts, subscriber counts)
- Newsletters that are low-signal or mass-market
- Duplicate notifications (same sender, same content pattern)
- Expired events, resolved alerts, stale login codes
- Service receipts and billing confirmations (already processed)
- Product update announcements from tools
- Mailing list messages with no personal content

KEEP if:
- Real person writing to Joel personally (colleagues, collaborators, friends)
- Active threads where someone is waiting for a reply (Re: prefix + real person)
- [aih] tagged messages (AI Hero collaboration ‚Äî always keep)
- Financial documents needing action (tax docs, unusual charges)
- Calendar invites from known contacts needing RSVP
- Content Joel specifically subscribes to and reads (The Information, Astral Codex Ten, Lenny's Newsletter, specific Substacks from people he knows like Kent Beck, Casey Newton)
- Anything ambiguous ‚Äî when in doubt, KEEP

Here are the conversations:

${listing}

Respond with ONLY valid JSON, no markdown fencing:
{"archive": ["cnv_xxx", "cnv_yyy"], "keep": ["cnv_zzz"]}`;

  try {
    // Write prompt to temp file to avoid shell escaping issues with large prompts
    const tmpFile = `/tmp/email-cleanup-prompt-${Date.now()}.txt`;
    require("fs").writeFileSync(tmpFile, prompt);

    const result = execSync(
      `pi --print --no-session --provider anthropic --model haiku "$(cat ${tmpFile})"`,
      {
        encoding: "utf-8",
        timeout: 120_000,
        stdio: ["pipe", "pipe", "pipe"],
      },
    ).trim();

    // Clean up temp file
    try { require("fs").unlinkSync(tmpFile); } catch {}

    // Find the JSON object in output ‚Äî skip any stderr noise that leaked through
    const lines = result.split("\n");
    for (const line of lines) {
      const trimmed = line.trim();
      if (trimmed.startsWith("{") && trimmed.includes('"archive"')) {
        try {
          return JSON.parse(trimmed);
        } catch {}
      }
    }

    // Fallback: try to extract any JSON object
    const jsonMatch = result.match(/\{"archive":\s*\[[\s\S]*?\].*?"keep":\s*\[[\s\S]*?\]\}/);
    if (jsonMatch) {
      return JSON.parse(jsonMatch[0]);
    }

    console.error("[email-cleanup] No valid JSON in pi response, keeping all. Output:", result.slice(0, 200));
    return { archive: [], keep: conversations.map((c) => c.id) };
  } catch (err) {
    console.error("[email-cleanup] pi inference failed, keeping all:", err);
    return { archive: [], keep: conversations.map((c) => c.id) };
  }
}

// ‚îÄ‚îÄ Main function ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

export const emailInboxCleanup = inngest.createFunction(
  {
    id: "email-inbox-cleanup",
    name: "Email ‚Üí Inbox Cleanup (AI Triage)",
    concurrency: [{ limit: 1 }], // only one cleanup at a time
  },
  { event: "email/inbox.cleanup" },
  async ({ event, step, ...rest }) => {
    const gateway = (rest as any).gateway as GatewayContext | undefined;
    const query = event.data?.query ?? "is:open";
    const maxPages = Math.min(event.data?.maxPages ?? MAX_PAGES, MAX_PAGES);
    const dryRun = event.data?.dryRun ?? false;

    // Step 1: Verify Front API access
    const frontToken = await step.run("get-front-token", () => {
      return getApiToken();
    });

    // Step 2: Paginate and classify
    let totalArchived = 0;
    let totalKept = 0;
    let totalProcessed = 0;
    let nextPageUrl: string | null = null;
    let pageNum = 0;

    while (pageNum < maxPages) {
      const page = pageNum;

      // Fetch a page of conversations
      const pageData = await step.run(`fetch-page-${page}`, async () => {
        const url = nextPageUrl
          ? nextPageUrl
          : `/conversations/search/${encodeURIComponent(query)}?limit=${PAGE_SIZE}`;

        const data = await frontGet(url, frontToken);
        const conversations: ConversationSummary[] = (
          data._results ?? []
        ).map((c: any) => ({
          id: c.id,
          subject: (c.subject ?? "(no subject)").slice(0, 100),
          from: (c.recipient?.name ?? "").slice(0, 40),
          email: c.recipient?.handle ?? "unknown",
          date: new Date(
            (c.last_message_at ?? c.created_at ?? 0) * 1000,
          )
            .toISOString()
            .slice(0, 10),
        }));

        return {
          conversations,
          total: data._total ?? 0,
          nextUrl: data._pagination?.next ?? null,
        };
      });

      if (pageData.conversations.length === 0) break;
      nextPageUrl = pageData.nextUrl;

      // Classify in batches of BATCH_SIZE using pi CLI
      const batches: ConversationSummary[][] = [];
      for (let i = 0; i < pageData.conversations.length; i += BATCH_SIZE) {
        batches.push(pageData.conversations.slice(i, i + BATCH_SIZE));
      }

      for (let b = 0; b < batches.length; b++) {
        const batch = batches[b];

        const classification = await step.run(
          `classify-page-${page}-batch-${b}`,
          () => {
            return classifyBatch(batch);
          },
        );

        const archiveIds = classification.archive ?? [];
        const keepIds = classification.keep ?? [];

        // Archive unless dry run
        if (!dryRun && archiveIds.length > 0) {
          await step.run(`archive-page-${page}-batch-${b}`, async () => {
            let archived = 0;
            let errors = 0;
            for (const id of archiveIds) {
              try {
                const res = await fetch(
                  `${FRONT_API}/conversations/${id}`,
                  {
                    method: "PATCH",
                    headers: {
                      Authorization: `Bearer ${frontToken}`,
                      "Content-Type": "application/json",
                    },
                    body: JSON.stringify({ status: "archived" }),
                  },
                );
                if (res.ok || res.status === 204) {
                  archived++;
                } else {
                  errors++;
                }
              } catch {
                errors++;
              }
            }
            return { archived, errors };
          });
        }

        totalArchived += archiveIds.length;
        totalKept += keepIds.length;
        totalProcessed += batch.length;
      }

      pageNum++;
      if (!nextPageUrl) break;

      // Breathing room for rate limits
      if (pageNum < maxPages && nextPageUrl) {
        await step.sleep(`rate-limit-pause-${page}`, "2s");
      }
    }

    // Final summary
    const summary = {
      query,
      dryRun,
      pagesProcessed: pageNum,
      totalProcessed,
      totalArchived,
      totalKept,
    };

    // Notify gateway with results
    await step.run("notify-results", async () => {
      if (!gateway) return { pushed: false };

      const emoji = dryRun ? "üîç" : "üßπ";
      const action = dryRun ? "DRY RUN" : "COMPLETED";

      return await gateway.notify("email.inbox.cleanup", {
        prompt: [
          `## ${emoji} Inbox Cleanup ${action}`,
          "",
          `**Query**: \`${query}\``,
          `**Processed**: ${totalProcessed} conversations`,
          `**Archived**: ${totalArchived}`,
          `**Kept**: ${totalKept}`,
          `**Pages**: ${pageNum}`,
          "",
          dryRun
            ? "This was a dry run. Re-run with `dryRun: false` to execute."
            : "Archive complete. Run `joelclaw email inbox` to check remaining.",
        ].join("\n"),
      });
    });

    return summary;
  },
);
